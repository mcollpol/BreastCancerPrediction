{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance analysis is a critical aspect of understanding the relevance and influence of variables in predicting the target variable within a machine learning model. This analysis serves multiple purposes: it identifies key features, assists in the selection process to mitigate computational complexity, provides insights into the relationships between features and the target variable, and ultimately aids in explaining the model's behavior, facilitating informed decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the feature importance with a Random Forest Classifier. Random Forest calculates feature importance during training by considering how features contribute to the model's ability to make accurate predictions. Features that are important for making predictions are more likely to be selected for splitting nodes in the trees, leading to greater impurity decrease and higher feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = t_tmp.drop(columns=[target])\n",
    "y = t_tmp[target]\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns,\n",
    "                                      'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance',\n",
    "                                                          ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'],\n",
    "         feature_importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed during the Correlation Analysis, the most significant features to determine the diagnosis are, in order of significance, *radius* worst, *concave points* in its mean and worst forms, *perimeter* worst and *area* worst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate feature importance using a technique called Permutation feature importance. When computing permutation importance, the model's performance metric (e.g., accuracy, mean squared error) is evaluated before and after shuffling the values of a feature. The difference between these two performances indicates the importance of the feature: if shuffling the feature values leads to a significant decrease in performance, the feature is considered important. In this case the already trained Random Forest Classifier will serve as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "perm_importance = permutation_importance(rf_classifier,\n",
    "                                         X,\n",
    "                                         y,\n",
    "                                         n_repeats=10,\n",
    "                                         random_state=42)\n",
    "\n",
    "feature_names = X.columns\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names[sorted_idx],\n",
    "         perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that for the specific case of a Random Forest Classifier, the most significant variables are *concave points_mean*, *area_se*, *texture_worst* and *concave points_worst*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree-based model like Random Forest may assign higher importance to certain features compared to a linear model like Logistic Regression. This difference arises because decision trees split the data based on feature values, making certain features more influential for prediction, whereas linear models estimate coefficients for each feature based on their relationship with the target variable. Let's redo the analysis using a logistic regression classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform feature importance analysis using Logistic Regression, the coefficients of the logistic regression model will serve as a measure of feature importance. Features with higher absolute coefficients are considered more important for predicting the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X, y)\n",
    "\n",
    "\n",
    "feature_coeff = logreg_model.coef_[0]\n",
    "\n",
    "# Create a DataFrame with feature names and their coefficients\n",
    "feature_coeff_df = pd.DataFrame({'Feature': X.columns,\n",
    "                                 'Coefficient': feature_coeff})\n",
    "\n",
    "# Sort the DataFrame by absolute coefficient values\n",
    "feature_coeff_df['Abs_Coefficient'] = abs(feature_coeff_df['Coefficient'])\n",
    "feature_coeff_df = feature_coeff_df.sort_values(by='Abs_Coefficient',\n",
    "                                                ascending=False)\n",
    "\n",
    "# Plot feature coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_coeff_df['Feature'],\n",
    "         feature_coeff_df['Coefficient'])\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Logistic Regression Feature Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables with higher coefficients are *texture_worst*, *area_worst*, *area_se*, *concavity_worst*, *smoothness_worst*, *texture_mean*, *compactness_worst* and *symmetry_worst*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the permutation importance technique with our Logistic Regression Classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "perm_importance = permutation_importance(logreg_model,\n",
    "                                         X,\n",
    "                                         y,\n",
    "                                         n_repeats=10,\n",
    "                                         random_state=42)\n",
    "\n",
    "feature_names = X.columns\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names[sorted_idx],\n",
    "         perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that for this type of classifier, there are more variables that contribute at model's predictions. From them, the most relevant ones are *area_se*, *area_worst*, *compactness_worst*, *compactness_se*, *texture_worst* and *texture_mean*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From both analysis, the less significant variables that **could be excluded** during feature selection, are: **All SE variables** but *area_se*, ***fractal_dimension_mean***, ***symmetry_mean***, ***compactness_mean***, ***smoothness_mean***, ***compactness_mean*** and ***fractal_dimension_worst***."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
